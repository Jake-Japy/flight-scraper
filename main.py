from fastapi import FastAPI, Depends, HTTPException
from sqlalchemy.orm import Session
from pydantic import BaseModel
from datetime import datetime
import requests
from celery import Celery
from models import Flight
import redis
import json
import logging
from tenacity import retry, stop_after_attempt, wait_fixed
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from spiders.flight_spider import FlightSpider
from scrapy import signals
from database import engine, SessionLocal, Base  # Import from database.py
import asyncio

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

app = FastAPI()

# Set up the redis client
try:
    redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    redis_client.ping()
except redis.ConnectionError as e:
    logger.warning(f"Redis connection failed: {e}. Proceeding without cache.")
    redis_client = None

# Set up the celery queue for task queueing
celery = Celery('tasks', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0')
celery.conf.update(task_track_started=True)

# Build the database models and output to server on uvicorn start
Base.metadata.create_all(bind=engine)
logger.info("Database tables created")

# Pydantic model used for type enforcement on generating requests to prevent user errors or possible type error conversions
class FlightRequest(BaseModel):
    airline_code: str
    flight_number: str
    departure_date: str  # Format: YYYY-MM-DD

# Pydantic model used to enforce likely response types generated by the request. Needed to ensure database type
# compliance. Could potentially look at an unstructured data type in future to remove the need for type compliance.
class FlightResponse(BaseModel):
    flight_id: str
    airline_code: str
    flight_number: str
    departure_date: str
    departure_airport: str
    arrival_airport: str
    departure_time: str
    arrival_time: str
    status: str
    gate: str | None

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# A decorator needed to alter the request to attempt a retry
@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
def make_request(url, headers):
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return response

# Celery task decorator to create the scraping task through scrapy.
@celery.task
def scrape_flight_data(airline_code: str, flight_number: str, departure_date: str):
    logger.debug(f"Scraping flight {airline_code}{flight_number} for {departure_date}")
    items = []

    def item_scraped_handler(item, response, spider):
        items.append(item)

    process = CrawlerProcess(get_project_settings())
    crawler = process.create_crawler(FlightSpider)
    crawler.signals.connect(item_scraped_handler, signal=signals.item_scraped)
    process.crawl(crawler, airline_code=airline_code, flight_number=flight_number, departure_date=departure_date)
    process.start()

    # Below I attempt to contact the server and generate a response that I can use to build the data.
    if items:
        flight_data = items[0]
        if "error" in flight_data:
            return flight_data

        # Ensure departure_date is datetime for database
        if not isinstance(flight_data['departure_date'], datetime):
            try:
                flight_data['departure_date'] = datetime.strptime(flight_data['departure_date'], "%Y-%m-%d")
            except ValueError as e:
                logger.error(f"Invalid departure date format: {e}")
                return {"error": f"Invalid departure date format: {departure_date}"}

        # Save to database
        db = SessionLocal()
        try:
            db_flight = Flight(**flight_data)
            db.add(db_flight)
            db.commit()
            db.refresh(db_flight)
        except Exception as e:
            logger.error(f"Database error: {e}")
            raise
        finally:
            db.close()
        # Prepare serializable flight_data for caching and response
        flight_data_serializable = {
            **flight_data,
            "departure_date": flight_data["departure_date"].strftime("%Y-%m-%d")
        }
        # Cache the result if Redis is available
        if redis_client:
            try:
                redis_client.setex(f"flight:{airline_code}:{flight_number}:{departure_date}", 300,
                                   json.dumps(flight_data_serializable))
            except redis.ConnectionError:
                logger.warning("Failed to cache in Redis, proceeding without cache.")

        logger.debug(f"Successfully scraped and stored flight {airline_code}{flight_number}")
        return flight_data_serializable
    else:
        logger.error(f"Scraping error: No data returned for {airline_code}{flight_number} on {departure_date}")
        return {"error": f"Failed to parse flight data: No data returned"}

@app.get("/flights/", response_model=FlightResponse)
async def get_flight_info(airline_code: str, flight_number: str, departure_date: str, db: Session = Depends(get_db)):
    # Validate departure_date format must be in a year month day format - I might need to consider some alternatives for
    # the American way of inputting dates in month day year format
    try:
        datetime.strptime(departure_date, "%Y-%m-%d")
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid date format. Use YYYY-MM-DD")

    cache_key = f"flight:{airline_code}:{flight_number}:{departure_date}"
    cached_data = None
    # check that redis is accepting connections (if not check port number / or if it is running)
    if redis_client:
        try:
            cached_data = redis_client.get(cache_key)
            if cached_data:
                logger.debug(f"Cache hit for {cache_key}")
                return json.loads(cached_data)
        except redis.ConnectionError:
            logger.warning("Redis unavailable, skipping cache.")

    flight = db.query(Flight).filter(
        Flight.airline_code == airline_code,
        Flight.flight_number == flight_number,
        Flight.departure_date == departure_date
    ).first()

    if flight:
        flight_data = {
            "flight_id": flight.flight_id,
            "airline_code": flight.airline_code,
            "flight_number": flight.flight_number,
            "departure_date": flight.departure_date.strftime("%Y-%m-%d"),
            "departure_airport": flight.departure_airport,
            "arrival_airport": flight.arrival_airport,
            "departure_time": flight.departure_time,
            "arrival_time": flight.arrival_time,
            "status": flight.status,
            "gate": flight.gate
        }
        if redis_client:
            try:
                redis_client.setex(cache_key, 3600, json.dumps(flight_data))
            except redis.ConnectionError:
                logger.warning("Failed to cache in Redis, proceeding without cache.")
        logger.debug(f"Database hit for {cache_key}")
        return flight_data

    logger.debug(f"Starting Celery task for {airline_code}{flight_number} on {departure_date}")
    # Creates the task that gets pushed into the celery queue
    try:
        task = scrape_flight_data.delay(airline_code, flight_number, departure_date)
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(None, task.get, 30)
        if "error" in result:
            logger.error(f"Celery task returned error: {result['error']}")
            raise HTTPException(status_code=404, detail=f"Flight data retrieval failed: {result['error']}")
        return result
    # if celery is malfunctioning for some reason this will allow the scraping to occur without it being queued.
    # if this step fails while you are attempting multiple requests it could cause requests to be missed or issues
    # with the synchronicity of your requests. It is advisable you run this with celery enabled and running.
    except Exception as e:
        logger.error(f"Celery task failed: {str(e)}")
        logger.info("Falling back to direct scraping due to Celery failure")
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(None, scrape_flight_data, airline_code, flight_number, departure_date)
        if "error" in result:
            logger.error(f"Direct scraping failed: {result['error']}")
            raise HTTPException(status_code=404, detail=f"Flight data retrieval failed: {result['error']}")
        return result